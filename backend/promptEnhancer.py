"""
===============================================================================
PROMPT ENHANCER - AM√âLIORATION AUTOMATIQUE DES PROMPTS RFP
===============================================================================

üîó CONNEXION AVEC muRag_vlm.py:
-------------------------------
Ce module travaille en synergie avec muRag_vlm.py pour am√©liorer automatiquement
les prompts d'analyse RFP en utilisant les √©valuations de fid√©lit√© g√©n√©r√©es.

üìä FLUX DE DONN√âES:
------------------
1. muRag_vlm.py effectue une analyse RFP et g√©n√®re:
   - last_evaluation_score: Score de fid√©lit√© (0.0 √† 1.0)
   - last_evaluation_reason: Raison d√©taill√©e de l'√©valuation
   - last_analysis_prompt: Prompt utilis√© pour l'analyse

2. promptEnhancer.py r√©cup√®re ces donn√©es et les utilise pour:
   - Identifier les faiblesses du prompt original
   - G√©n√©rer un prompt am√©lior√© avec Gemini 2.5 Flash
   - Cibler un score de fid√©lit√© plus √©lev√©

üéØ OBJECTIF:
-----------
Cr√©er un cycle d'am√©lioration continue o√π chaque analyse RFP permet
d'optimiser les futurs prompts pour de meilleurs r√©sultats.

üöÄ USAGE SIMPLIFI√â:
------------------
1. Lancez muRag_vlm.py avec un domaine (ex: "s√©curit√©")
2. Lancez promptEnhancer.py ‚Üí am√©lioration automatique !
3. Utilisez le prompt am√©lior√© pour de futures analyses

"""

from muRag_vlm import last_evaluation_score, last_evaluation_reason, last_analysis_prompt

"""
===============================================================================
CONFIGURATION API GEMINI
===============================================================================
"""

import os, getpass
GOOGLE_API_KEY = "AIzaSyCEtBwwTcJitPxX6Vum92Bz6q5-Ga86hU4"
if not GOOGLE_API_KEY or GOOGLE_API_KEY == "PASTE_YOUR_KEY_HERE":
    raise ValueError("‚ùå Please set your Gemini API key")
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
print("‚úÖ Key set")

"""
===============================================================================
MOTEUR D'AM√âLIORATION DE PROMPTS
===============================================================================
"""

import google.generativeai as genai, textwrap, os

genai.configure(api_key=os.environ['GOOGLE_API_KEY'])
_enhancer_llm = genai.GenerativeModel('gemini-2.5-flash')

def enhance_prompt(predefined_prompt: str, judge_reports: list[dict], target_score: float = 0.7) -> str:
    """
    üîß FONCTION CORE: Am√©lioration de prompt via Gemini
    
    Utilise les retours d'√©valuation de muRag_vlm.py pour r√©crire un prompt
    et maximiser la pertinence des preuves r√©cup√©r√©es.
    
    Args:
        predefined_prompt: Prompt original √† am√©liorer
        judge_reports: Rapports d'√©valuation de muRag_vlm.py (score + raison)
        target_score: Score de fid√©lit√© cible √† atteindre
    
    Returns:
        str: Prompt r√©√©crit et optimis√©
    """
    feedback_lines = []
    for r in judge_reports:
        feedback_lines.append(f"‚Ä¢ {r['judge_type'].capitalize()} {r['score']:.2f}: {r['reason']}")
    feedback_block = "\n".join(feedback_lines)

    system_p = textwrap.dedent("""As an expert prompt engineer in a Retrieval-Augmented Generation pipeline, your job is to rewrite user queries to achieve two main objectives:

        *Maximize the relevance of the retrieved evidence.
        *Ensure the generated answer satisfies all downstream judges by being accurate, complete, and directly addressing the query.
To do this effectively, consider the following strategies when rewriting:

    -Clarify the intent of the query if it is ambiguous.
    -Provide additional context that might help in retrieving pertinent information.
    -Specify key terms or aspects that should be focused on.
    -Eliminate any unnecessary or misleading parts of the query.
Output only the rewritten query, nothing else.""")
    user_p = f"""predefined_prompt:
{predefined_prompt}

Judge feedback:
{feedback_block}

Goal: rewrite the prompt so that every judge would likely score ‚â• {target_score}."""
    rsp = _enhancer_llm.generate_content(system_p + "\n" + user_p)
    return rsp.text.strip()

print('‚úÖ Prompt‚ÄëEnhancement function ready.')

def enhance_from_murag_evaluation(user_prompt: str = None, target_score: float = 0.7) -> str:
    """
    üöÄ FONCTION PRINCIPALE: Interface simplifi√©e avec muRag_vlm.py
    
    Cette fonction √©tablit le pont automatique entre muRag_vlm.py et promptEnhancer.py.
    Elle r√©cup√®re automatiquement les derni√®res √©valuations g√©n√©r√©es par muRag_vlm.py
    et les utilise pour am√©liorer un prompt.
    
    üîó CONNEXION AUTOMATIQUE:
    ------------------------
    - R√©cup√®re last_evaluation_score de muRag_vlm.py
    - R√©cup√®re last_evaluation_reason de muRag_vlm.py  
    - R√©cup√®re last_analysis_prompt de muRag_vlm.py (si aucun prompt fourni)
    
    Args:
        user_prompt (str, optional): Le prompt √† am√©liorer. 
                                   Si None, utilise automatiquement last_analysis_prompt
        target_score (float): Score de fid√©lit√© cible souhait√© (d√©faut: 0.7)
        
    Returns:
        str: Prompt am√©lior√© bas√© sur l'√©valuation de muRag_vlm.py
        
    Exemple d'usage:
        # Apr√®s avoir lanc√© muRag_vlm.py avec "s√©curit√©"
        enhanced = enhance_from_murag_evaluation()  # Utilise tout automatiquement !
    """
    # Si aucun prompt fourni, utiliser le dernier prompt d'analyse de muRag_vlm.py
    if user_prompt is None:
        if last_analysis_prompt is not None:
            user_prompt = last_analysis_prompt
            print(f"üìù Utilisation du prompt d'analyse automatique: {last_analysis_prompt}")
        else:
            print("‚ö†Ô∏è Aucun prompt fourni et aucun prompt d'analyse disponible")
            return "Aucun prompt disponible"
    
    # V√©rifier si on a des √©valuations disponibles depuis muRag_vlm.py
    if last_evaluation_score is None or last_evaluation_reason is None:
        print("‚ö†Ô∏è Aucune √©valuation disponible dans muRag_vlm.py")
        print("üí° Conseil: Lancez d'abord muRag_vlm.py pour g√©n√©rer des √©valuations")
        return user_prompt
    
    # Cr√©er le rapport d'√©valuation avec les donn√©es de muRag_vlm.py
    judge_reports = [
        {"judge_type": "faithfulness", "score": last_evaluation_score, "reason": last_evaluation_reason}
    ]
    
    # Am√©liorer le prompt avec Gemini
    enhanced = enhance_prompt(user_prompt, judge_reports, target_score)
    
    print(f"üìä Score actuel: {last_evaluation_score:.2f}")
    print(f"üéØ Score cible: {target_score}")
    print(f"üí° Raison: {last_evaluation_reason}")
    
    return enhanced

"""
===============================================================================
D√âMONSTRATION DU LIEN AVEC muRag_vlm.py
===============================================================================

Ce bloc d√©montre les 3 fa√ßons d'utiliser les donn√©es g√©n√©r√©es par muRag_vlm.py:

üîÑ WORKFLOW COMPLET:
1. Utilisateur lance muRag_vlm.py et tape "s√©curit√©"
2. muRag_vlm.py g√©n√®re une analyse et stocke:
   - last_analysis_prompt = "Analyse strat√©gique compl√®te des RFP sur le domaine s√©curit√©"
   - last_evaluation_score = 0.65 (exemple)
   - last_evaluation_reason = "L'analyse manque de sp√©cificit√©s techniques"
3. promptEnhancer.py r√©cup√®re ces donn√©es automatiquement
4. Gemini g√©n√®re un prompt am√©lior√© pour atteindre un score > 0.7

"""

print("\nüöÄ D√âMONSTRATION AVEC LES √âVALUATIONS DE muRag_vlm.py:")
print("="*60)

# ============================================================================
# M√âTHODE 1: AUTOMATIQUE (RECOMMAND√âE)
# ============================================================================
# Utilise automatiquement TOUT ce qui a √©t√© g√©n√©r√© par muRag_vlm.py:
# - Le prompt d'analyse (last_analysis_prompt)
# - Le score d'√©valuation (last_evaluation_score)  
# - La raison d'√©valuation (last_evaluation_reason)
enhanced_prompt = enhance_from_murag_evaluation()  # Pas besoin de fournir le prompt !
print(f"\n‚ú® PROMPT AM√âLIOR√â AUTOMATIQUE:\n{enhanced_prompt}")

print("\n" + "="*60)

# ============================================================================
# M√âTHODE 2: SEMI-AUTOMATIQUE 
# ============================================================================
# Fournit un prompt personnalis√© mais utilise les √©valuations de muRag_vlm.py
custom_prompt = "Analysez les facteurs de succ√®s dans nos r√©ponses RFP"
enhanced_custom = enhance_from_murag_evaluation(custom_prompt)
print(f"\nüéØ PROMPT PERSONNALIS√â AM√âLIOR√â:\n{enhanced_custom}")

print("\n" + "="*60)

# ============================================================================
# M√âTHODE 3: MANUELLE (CONTR√îLE TOTAL)
# ============================================================================
# Acc√®s direct aux variables de muRag_vlm.py pour un contr√¥le complet
if last_evaluation_score is not None and last_evaluation_reason is not None and last_analysis_prompt is not None:
    judge_reports = [
        {"judge_type": "faithfulness", "score": last_evaluation_score, "reason": last_evaluation_reason}
    ]
    manual_enhanced = enhance_prompt(last_analysis_prompt, judge_reports)
    print(f"\nüîß M√âTHODE MANUELLE:\n{manual_enhanced}")
else:
    print("\n‚ö†Ô∏è Donn√©es incompl√®tes - lancez d'abord muRag_vlm.py pour une analyse compl√®te")
    print("üí° √âTAPES √Ä SUIVRE:")
    print("   1. Ouvrez muRag_vlm.py")  
    print("   2. Tapez un domaine (ex: 's√©curit√©', 'cloud', 'agile')")
    print("   3. Attendez la fin de l'analyse avec √©valuation")
    print("   4. Relancez promptEnhancer.py ‚Üí am√©lioration automatique !")

